{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Metrics Onboarding Notebook \n",
    "#### Author : [@Achintya](https://github.com/AchintyaX)\n",
    "\n",
    "In this Notebook we would be calculating the metrics for which we monitor in ML weekly review along with metrics for evaluation the performance of our model before deployment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing the necessary libraries "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, precision_score, recall_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the data from the tog job\n",
    "The data that we retrive form a tog job is in a sqlite file, we have standard way of encoding the data inside a `data` table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def load_sqlite(path):\n",
    "    cnx = sqlite3.connect(path)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM data\", cnx)\n",
    "    return df "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = load_sqlite('982.sqlite')\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extracting Labels and Predictions \n",
    "Inside the `df` we have the `tag` and `data` columns. <br>\n",
    "The `tag` column contains the intent tags labelled by the Ops team(ground truth labels)<br>\n",
    "the `data` column contains the predictions from our bot.(Predictions) <br>\n",
    "Both are in Json format so we need to extract the prediction and label by parsing the json. <br>\n",
    "the code block below performs that function, please feel free to explore the data further "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def predictions_labels(row):\n",
    "    try:\n",
    "        prediction = json.loads(row['data'])['filter']['predicted_intent']\n",
    "    except KeyError:\n",
    "        prediction = '_empty_transcript_'\n",
    "    label = json.loads(row['tag'])[0]['type']\n",
    "    return prediction, label "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels = []\n",
    "predictions = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    prediction, label = predictions_labels(row)\n",
    "    predictions.append(prediction)\n",
    "    labels.append(label)\n",
    "\n",
    "df['prediction'] = predictions\n",
    "df['label'] = labels \n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking out the list of labels present inside our tog job"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "df.label.unique()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['_silence_', '_callback_', '_purchased_', '_ood_', '_hindi_',\n",
       "       '_pending_', '_cancel_', '_uninterested_', '_confirm_',\n",
       "       '_who_is_this_', 'two_wheeler'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distribution of Intents \n",
    "We can divide the intents into 2 groups - \n",
    "1. Inscope Intents - Intents which are in the use case of the bot \n",
    "2. Out of Scope Intents - Intents which our out of the use case of the bot. These are represented by `_oos_` but there could be other intents which can be aliased to `_oos_`\n",
    "\n",
    "Inside the Inscope Intents we have another group of intents called `smalltalk intents`. <br>\n",
    "- Smalltalk Intents : These are the intents which are part of every bot, basically if the user is trying to make a smalltalk with the bot. generally the following intents are in smalltalk. \n",
    "    - `_confirm_`\n",
    "    - `_cancel_`\n",
    "    - `_repeat_`\n",
    "    - `_greeting_`\n",
    "    \n",
    "<br>\n",
    "The list can expand depending on the client "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# we don't need _silence_ intent because that is for silent audios\n",
    "\n",
    "df = df[df['label'] != '_silence_'].reset_index(drop=True)\n",
    "\n",
    "smalltalk_intents = ['_confirm_', '_cancel_', '_repeat_', '_greeting_']\n",
    "inscope_intents = [i for i in df.label.unique().tolist() if i not in ['_oos_', '_ood_']]\n",
    "inscope_without_smalltalk = [i for i in inscope_intents if i not in smalltalk_intents]\n",
    "oos = ['_oos_']\n",
    "\n",
    "def scorer(df, intents=None, average=\"weighted\"):\n",
    "    if intents:\n",
    "        df = df[df['label'].isin(intents)].reset_index(drop=True)\n",
    "    score = {}\n",
    "    score['precision'] = precision_score(df['label'], df['prediction'], average='weighted', zero_division=0)\n",
    "    score['recall'] = recall_score(df['label'], df['prediction'], average='weighted', zero_division=0)\n",
    "    return score "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metric Calculation - \n",
    "As mentioned in the [doc](https://docs.google.com/document/d/1txL6Dq5qQdfvYxdU_3Z-Z6Rfu0WV_A_SgQz4EdVDEFM/edit#heading=h.6mcxiaz7ktcb), these are the following metrics that we monitor in ML weekly review - \n",
    "1. Inscope Precision - Precision of all inscope Intents \n",
    "2. Inscope Recall - Recall of all inscope Intents \n",
    "3. Smalltalk Precicion - Precision of only Smalltalk intents \n",
    "4. Smalltalk Recall - Recall of only Smalltalk intents \n",
    "5. Inscope precision without smalltalk \n",
    "6. Inscope Recall without Smalltalk \n",
    "7. Slot Capture Rate \n",
    "8. Slot Retry Rate \n",
    "\n",
    "The details and procedure for calculating slot related metrics can be found [here](https://github.com/Vernacular-ai/onboarding/blob/master/ml/slot-reporting/slot-evaluation-and-reporting.ipynb)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(\"Overall Precision is {} and overall recall is {}\".format(scorer(df)['precision'], scorer(df)['recall']))\n",
    "print(\"Smalltalk Precision is {} and Smalltalk Recall is {}\".format(scorer(df, smalltalk_intents)['precision'], scorer(df, smalltalk_intents)['recall']))\n",
    "print(\"Inscope Precision is {} and Inscope Recall is {}\".format(scorer(df, inscope_intents)['precision'], scorer(df, inscope_intents)['recall']))\n",
    "print(\"Inscope Precision without smalltalk is {} and Inscope Recall without Smalltalk is {}\".format(scorer(df, inscope_without_smalltalk)['precision'], scorer(df, inscope_without_smalltalk)['recall']))\n",
    "print(\"OOS precision is {} and OOS recall is {}\".format(scorer(df, oos)['precision'], scorer(df, oos)['recall']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overall Precision is 0.9623137108792846 and overall recall is 0.8114754098360656\n",
      "Smalltalk Precision is 1.0 and Smalltalk Recall is 0.875\n",
      "Inscope Precision is 0.9703972082723714 and Inscope Recall is 0.8181818181818182\n",
      "Inscope Precision without smalltalk is 0.9946581196581197 and Inscope Recall without Smalltalk is 0.8162393162393162\n",
      "OOS precision is 0.0 and OOS recall is 0.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/achintya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/achintya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/achintya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/achintya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/achintya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/achintya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/achintya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/achintya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluating Model Performance \n",
    "For Evaluating the performance of our Model we generally use the classification report packaged with sklearn. Since our SLU is a classification model the key metric that we look for evaluating the performance is `F1 score`. <br>\n",
    "We use the classification report because it lets us analyze the performance of each intent "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(classification_report(df['label'], df['prediction'], zero_division=0))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "        _callback_       1.00      0.82      0.90       110\n",
      "          _cancel_       0.36      1.00      0.53         4\n",
      "         _confirm_       0.15      0.75      0.25         4\n",
      "_empty_transcript_       0.00      0.00      0.00         0\n",
      "           _hindi_       1.00      0.87      0.93        63\n",
      "      _interested_       0.00      0.00      0.00         0\n",
      "             _ood_       0.00      0.00      0.00         2\n",
      "  _other_language_       0.00      0.00      0.00         0\n",
      "         _pending_       1.00      0.68      0.81        25\n",
      "       _purchased_       1.00      0.60      0.75        10\n",
      "    _uninterested_       1.00      1.00      1.00        15\n",
      "     _who_is_this_       1.00      1.00      1.00         1\n",
      "       two_wheeler       0.88      0.70      0.78        10\n",
      "          why_what       0.00      0.00      0.00         0\n",
      "\n",
      "          accuracy                           0.81       244\n",
      "         macro avg       0.53      0.53      0.50       244\n",
      "      weighted avg       0.96      0.81      0.87       244\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/achintya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/achintya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/achintya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}